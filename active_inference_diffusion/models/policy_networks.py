"""
Policy Networks for Diffusion-Conditioned Active Inference
"""

import torch
import torch.nn as nn
import torch.distributions as dist
from typing import Tuple, Optional, List
import numpy as np  
import torch.nn.functional as F  

class DiffusionConditionedPolicy(nn.Module):
    """
    Gaussian policy conditioned on diffusion-generated latents
    Implements: p_φ(π|z) = N(π; μ_φ(z), Σ_φ(z))
    
    Key innovation: Policies emerge from the continuous latent manifold
    generated by the diffusion process
    """
    
    def __init__(
        self,
        latent_dim: int,
        action_dim: int,
        hidden_dim: int = 256,
        num_layers: int = 3,
        log_std_min: float = -20,
        log_std_max: float = 2,
        use_state_dependent_std: bool = True,
        squash_output: bool = False
    ):
        super().__init__()
        
        self.latent_dim = latent_dim
        self.action_dim = action_dim
        self.log_std_min = log_std_min
        self.log_std_max = log_std_max
        self.use_state_dependent_std = use_state_dependent_std
        self.squash_output = squash_output
        
        # Latent processing network with skip connections
        self.latent_encoder = nn.Sequential(
            nn.Linear(latent_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim)
        )
        
        # Policy trunk
        trunk_layers = []
        for i in range(num_layers):
            trunk_layers.extend([
                nn.Linear(hidden_dim, hidden_dim),
                nn.LayerNorm(hidden_dim),
                nn.ReLU()
            ])
        self.trunk = nn.Sequential(*trunk_layers)
        
        # Separate heads for mean and covariance
        self.mean_head = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, action_dim)
        )
        
        if use_state_dependent_std:
            self.log_std_head = nn.Sequential(
                nn.Linear(hidden_dim, hidden_dim // 2),
                nn.ReLU(),
                nn.Linear(hidden_dim // 2, action_dim)
            )
        else:
            # Learnable but state-independent std
            self.log_std = nn.Parameter(torch.zeros(action_dim))
            
        
    def forward(
        self,
        z: torch.Tensor,
        deterministic: bool = False
    ) -> Tuple[torch.Tensor, torch.Tensor, dist.Distribution]:
        """
        Generate policy distribution from diffusion latent
        
        Args:
            z: Diffusion-generated latent [batch_size, latent_dim]
            deterministic: If True, return mean action
            
        Returns:
            action: Sampled or mean action
            log_prob: Log probability of action
            distribution: Full policy distribution
        """
        # Process latent with skip connection
        h = self.latent_encoder(z)
        h = h + self.trunk(h)  # Residual connection
        
        # Compute mean
        mean = self.mean_head(h)
        
        # Compute std
        if self.use_state_dependent_std:
            log_std = self.log_std_head(h)
        else:
            log_std = self.log_std.expand_as(mean)
            
        log_std = torch.clamp(log_std, self.log_std_min, self.log_std_max)
        std = torch.exp(log_std)
        
        # Create distribution
        distribution = dist.Normal(mean, std)
        
        # Sample action
        if deterministic:
            action = mean
        else:
            action = distribution.rsample()
            
        # Apply squashing function if needed (for bounded action spaces)
        if self.squash_output:
            action = torch.tanh(action)
            # Correct log_prob for tanh squashing
            log_prob = distribution.log_prob(action).sum(dim=-1)
            log_prob -= (2 * (np.log(2) - action - F.softplus(-2 * action))).sum(dim=-1)
        else:
            log_prob = distribution.log_prob(action).sum(dim=-1)
            
        return action, log_prob, distribution
        
    def get_policy_entropy(self, z: torch.Tensor) -> torch.Tensor:
        """Compute policy entropy for exploration bonus"""
        _, _, distribution = self.forward(z, deterministic=True)
        return distribution.entropy().sum(dim=-1)


class HierarchicalDiffusionPolicy(nn.Module):
    """
    Hierarchical policy structure emerging from diffusion latents
    Implements temporal abstractions through latent dynamics
    """
    
    def __init__(
        self,
        latent_dim: int,
        action_dim: int,
        num_levels: int = 3,
        hidden_dim: int = 256
    ):
        super().__init__()
        
        self.num_levels = num_levels
        self.latent_dim = latent_dim
        
        # Create policy for each hierarchical level
        self.policies = nn.ModuleList([
            DiffusionConditionedPolicy(
                latent_dim=latent_dim,
                action_dim=action_dim if i == 0 else latent_dim,
                hidden_dim=hidden_dim,
                use_state_dependent_std=True
            )
            for i in range(num_levels)
        ])
        
        # Temporal abstraction networks
        self.temporal_encoders = nn.ModuleList([
            nn.LSTM(
                input_size=latent_dim,
                hidden_size=latent_dim,
                num_layers=1,
                batch_first=True
            )
            for _ in range(num_levels - 1)
        ])
        # Initialize policies
        for layer in self.policies.modules():
            if isinstance(layer, nn.Linear):
               nn.init.kaiming_normal_(layer.weight, mode='fan_in', nonlinearity='relu')
               nn.init.constant_(layer.bias, 0.0)
            elif isinstance(layer, nn.LayerNorm):
                nn.init.constant_(layer.weight, 1.0)
                nn.init.constant_(layer.bias, 0.0)
        # Initialize temporal encoders
        for layer in self.temporal_encoders:
            for name, param in layer.named_parameters():
                if 'weight_ih' in name:
                    nn.init.xavier_uniform_(param)
                elif 'weight_hh' in name:
                    nn.init.orthogonal_(param)
                elif 'bias' in name:
                    n = param.size(0)
                    param.data.fill_(0)
                    gate_size = n // 4
                    param.data[0:gate_size].fill_(1.0)

    def forward(
        self,
        z: torch.Tensor,
        level: int = 0,
        hidden_states: Optional[List[Tuple[torch.Tensor, torch.Tensor]]] = None
    ) -> Tuple[torch.Tensor, torch.Tensor, Optional[List[Tuple[torch.Tensor, torch.Tensor]]]]:
        """
        Hierarchical policy execution
        
        Higher levels generate subgoals in latent space,
        lower levels generate actions
        """
        if hidden_states is None:
            hidden_states = [None] * (self.num_levels - 1)
            
        current_z = z
        new_hidden_states = []
        
        # Process through hierarchy
        for i in range(self.num_levels - 1, level - 1, -1):
            if i < self.num_levels - 1:
                # Apply temporal abstraction
                z_seq = current_z.unsqueeze(1)  # Add time dimension
                encoded_z, hidden = self.temporal_encoders[i](z_seq, hidden_states[i])
                current_z = encoded_z.squeeze(1)
                new_hidden_states.append(hidden)
                
            # Get action/subgoal from current level
            action, log_prob, _ = self.policies[i](current_z)
            
            if i > level:
                # Use as subgoal for next level
                current_z = action
                
        return action, log_prob, new_hidden_states[::-1]